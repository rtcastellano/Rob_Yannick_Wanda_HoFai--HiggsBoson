---
title: "From Description to Ensemble"
author: "Robert Castellano, Yannick Kimmel, Wanda Wang, Ho Fai Wong"
output: 
  beamer_presentation:
    theme: "Berkeley"
---
\includegraphics[scale = .15]{Images/Higgs.jpg}

#First section title

#Models
##Our models

> - Random forest
> - Gbm
> - Xgboost

##Random forest model

\begin{itemize}
\item Tuning parameters
\begin{itemize}
  \item mtry: Number of splits per tree
\end{itemize}
\pause
\item Performed 5-fold CV to tune parameters.
\begin{itemize}
  \item mtry = 5
\end{itemize}
\pause
\item AUC on training data = .9071
\item Kaggle rank = 1311
\item AMS = 2.57949
\end{itemize}

##Random forest variable importance

\includegraphics[scale = .6]{Images/rfvarimp.jpeg}

##About xgboost

> - Fast gradient boosting algorithm implementing in C++ by Tianqi Chen

> - Parallel computing

> - More tuning parameters

> - Not completely greedy in tree creation

> - Generally faster and performs better than gbm.

##Xgboost model

\begin{itemize}
\item Parameters we tuned:
\begin{itemize}
  \item nrounds: Number of trees
  \item max\_depth
  \item colsample\_bytree: Percent of parameters used at each split.
  tree
  \item eta: Learning rate
\end{itemize}
\pause
\item Performed 5-fold CV to tune parameters.
\begin{itemize}
  \item nrounds = 200
  \item max\_depth = 5
  \item colsample\_bytree = .85
  \item eta = .2
\end{itemize}
\pause
\item AUC on training data = .9254
\item Kaggle rank = 1340
\item AMS = 2.49958
\end{itemize}


##Xgboost variable importance

\includegraphics[scale = .6]{Images/xgboost_var_importance.png}

## Ensemble

> - Combined three models by majority vote
> - Kaggle rank = 1309
> - AMS = 2.58510

#Room for improvement

##Feature engineering

\begin{itemize}
\item We did not include any additional variables
\begin{itemize}
\item Basic physics. e.g. Cartesian coordinates of momentum
\pause
\item Advanced physcics: e.g. CAKE variable
\pause
\item Better understand the physics of additional models
\end{itemize}
\pause
\item Log transforms
\end{itemize}

##Models

> - More models
> - More sophisticated emsemble
> - Run different random seeds for the same model
